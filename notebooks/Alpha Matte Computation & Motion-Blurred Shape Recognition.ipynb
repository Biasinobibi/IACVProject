{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-Form Matting with Scribbles\n",
    "\n",
    "## Alpha Matte Estimation from Scribbles User Input  \n",
    "*Based on the Python implementation of image matting method proposed in A. Levin D. Lischinski and Y. Weiss. A Closed Form Solution to Natural Image Matting. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2006, New York that can be found on https://github.com/MarcoForte/closed-form-matting*\n",
    "\n",
    "- biagio.cancelliere@mail.polimi.it\n",
    "- davide1.franchi@mail.polimi.it\n",
    "- carlos2.ruiz@mail.polimi.it\n",
    "\n",
    "This notebook provides an explanation of the mentioned **Closed-Form Matting** technique for natural images. The goal is to compute a continuous **alpha matte** $ \\alpha(x, y) \\in [0, 1] $, which represents the per-pixel opacity of the foreground object in a scene.\n",
    "\n",
    "Using sparse user annotations in the form of **scribbles**—indicating definite foreground and background regions—the method propagates the known alpha values across the image while preserving edges and local color statistics.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Dependencies](#setup-and-dependencies)\n",
    "3. [Data Preparation](#data-preparation)  \n",
    "   3.1 [Loading the Input Image and Scribbles](#loading-the-input-image-and-scribbles)  \n",
    "   3.2 [Visualizing the Inputs](#visualizing-the-inputs)\n",
    "4. [Alpha Prior from Scribbles](#alpha-prior-from-scribbles)\n",
    "5. [Matting Laplacian Construction](#matting-laplacian-construction)\n",
    "6. [Solving for the Alpha Matte](#solving-for-the-alpha-matte)\n",
    "7. [Result Visualization and Analysis](#result-visualization-and-analysis)\n",
    "8. [Conclusions and Further Reading](#conclusions-and-further-reading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Natural image matting is the process of estimating the opacity (alpha value) of the foreground object in an image, separating it from the background. This task is essential in many applications, including image editing, film production, augmented reality, and compositing.\n",
    "\n",
    "The **alpha matte** is a real-valued mask $ \\alpha(x, y) \\in [0, 1] $ that indicates the proportion of foreground present at each pixel:\n",
    "- $ \\alpha = 1 $: full foreground\n",
    "- $ \\alpha = 0 $: full background\n",
    "- $ \\alpha \\in (0,1) $: partial or mixed pixels (e.g., along object boundaries or in regions of transparency)\n",
    "\n",
    "Estimating an accurate alpha matte from a single image is highly ill-posed. To resolve this ambiguity, additional constraints are needed. One widely adopted approach is to use **user-provided annotations**, such as:\n",
    "- **Scribbles** indicating known foreground and background pixels\n",
    "- **Trimaps** that mark definite foreground, background, and unknown regions\n",
    "\n",
    "In this notebook, we focus on the method proposed by **Levin et al. (2008)**, which formulates image matting as a **closed-form solution** by leveraging the local color distribution of the image. Their method constructs a **Matting Laplacian**, a sparse matrix that enforces smoothness of the alpha matte within small windows under a color line model.\n",
    "\n",
    "Given a small set of user-defined scribbles, the method solves a sparse linear system to propagate alpha values across the image, producing a soft, edge-preserving alpha matte.\n",
    "\n",
    "This notebook walks through the complete pipeline:\n",
    "- Interpreting user scribbles as alpha constraints\n",
    "- Constructing the matting Laplacian\n",
    "- Solving for the alpha matte via linear system optimization\n",
    "- Visualizing and interpreting the resulting alpha mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies <a name=\"setup-and-dependencies\"></a>\n",
    "\n",
    "This section installs and imports the necessary Python libraries for image processing, numerical computation, and visualization.\n",
    "\n",
    "Make sure to run this cell before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already available)\n",
    "# Uncomment the following line if running in a clean environment (e.g., Colab)\n",
    "# !pip install opencv-python-headless scipy matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure plots for consistent visual output\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# Utility for displaying images\n",
    "def imshow(img, title=None, cmap=None):\n",
    "    if img.ndim == 2:\n",
    "        plt.imshow(img, cmap=cmap or \"gray\")\n",
    "    else:\n",
    "        # OpenCV uses BGR; convert to RGB for display\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation <a name=\"data-preparation\"></a>\n",
    "\n",
    "To compute the alpha matte, we need two input images:\n",
    "\n",
    "- The **original RGB image** containing the object to be segmented.\n",
    "- A corresponding **scribbles image**, where the user has marked some pixels as definite foreground (typically white) and definite background (typically black).\n",
    "\n",
    "The scribbles act as soft constraints that guide the matting algorithm.\n",
    "\n",
    "The input and scribbles images must:\n",
    "- Have the same dimensions\n",
    "- Be in color (3-channel RGB/BGR)\n",
    "- Be properly aligned\n",
    "\n",
    "We will now load both images and display them to verify their correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the Input Image and Scribbles <a name=\"loading-the-input-image-and-scribbles\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "def _resize_pair(img: np.ndarray, scrib: np.ndarray, max_dim: Union[int, None]) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    If max_dim is not None and either dimension exceeds max_dim,\n",
    "    compute a scale factor and resize both img and scrib to that scale,\n",
    "    preserving aspect ratio. Returns (img_resized, scrib_resized, scale_factor).\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    if max_dim is None or max(h, w) <= max_dim:\n",
    "        return img, scrib, 1.0\n",
    "\n",
    "    scale = max_dim / max(h, w)\n",
    "    new_size = (int(w * scale), int(h * scale))\n",
    "    img_resized = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "    scrib_resized = cv2.resize(scrib, new_size, interpolation=cv2.INTER_AREA)\n",
    "    return img_resized, scrib_resized, scale\n",
    "\n",
    "# Load scraper image and scribbles\n",
    "image_path = \"../assets/images/IMG_1143.png\"\n",
    "scribbles_path = \"../assets/scribbles/flyingball.png.png\"\n",
    "\n",
    "# Load flyingball image and scribbles\n",
    "image_path = \"../assets/images/flyingball.png\"\n",
    "scribbles_path = \"../assets/scribbles/flyingball_scribbles.png\"\n",
    "\n",
    "# Load fan image and scribbles (invert the path definitions for changing images)\n",
    "image_path = \"../assets/images/IMG_9264.png\"\n",
    "scribbles_path = \"../assets/scribbles/IMG_9264_scribbles_hard.png\"\n",
    "\n",
    "image_raw = cv2.imread(image_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "scribbles_raw = cv2.imread(scribbles_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "\n",
    "# Sanity check\n",
    "assert image_raw is not None and scribbles_raw is not None, \"Failed to load input images\"\n",
    "assert image_raw.shape == scribbles_raw.shape, \"Image and scribbles must have the same dimensions\"\n",
    "\n",
    "# Resize if necessary\n",
    "max_dim = 512  # Set maximum size for either dimension\n",
    "image, scribbles, scale = _resize_pair(image_raw, scribbles_raw, max_dim)\n",
    "\n",
    "print(f\"Original shape: {image_raw.shape[:2]}, resized to: {image.shape[:2]}, scale factor: {scale:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing the Inputs <a name=\"visualizing-the-inputs\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original image\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(image, \"Original Image\")\n",
    "plt.show()\n",
    "\n",
    "# Display scribbles overlay\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(scribbles, \"Scribbles Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alpha Prior from Scribbles <a name=\"alpha-prior-from-scribbles\"></a>\n",
    "\n",
    "The matting algorithm requires an initial estimate of the alpha values—called the **alpha prior**—to guide the solution. This prior is derived directly from the scribbles image provided by the user.\n",
    "\n",
    "### Interpretation of Scribbles\n",
    "\n",
    "We assume the scribbles image consists of:\n",
    "- **Black strokes** (RGB ≈ [0, 0, 0]) marking definite background\n",
    "- **White strokes** (RGB ≈ [1, 1, 1]) marking definite foreground\n",
    "- **Unmodified areas** (same as original image) representing unknown regions\n",
    "\n",
    "We compute the alpha prior $\\alpha_0(x, y)$ for each pixel using the formula:\n",
    "\n",
    "$$\n",
    "\\alpha_0(x, y) = \\frac{1}{2} \\cdot \\text{sign} \\left( \\sum_{c=1}^{3} (S_c(x, y) - I_c(x, y)) \\right) + 0.5\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ S(x, y) $: the scribbles image\n",
    "- $ I(x, y) $: the original image\n",
    "- The result maps to:\n",
    "  - $ \\alpha_0 = 1 $: foreground (white scribble)\n",
    "  - $ \\alpha_0 = 0 $: background (black scribble)\n",
    "  - $ \\alpha_0 = 0.5 $: unknown (no scribble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alpha prior from scribbles\n",
    "prior = np.sign(np.sum(scribbles - image, axis=2)) / 2.0 + 0.5\n",
    "\n",
    "# Define constant mask: where prior is either 0 or 1 (i.e., where user actually painted)\n",
    "consts_map = (prior != 0.5).astype(np.float32)\n",
    "\n",
    "# Display prior\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(prior, \"Alpha Prior (from Scribbles)\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally visualize where scribbles exist (binary mask)\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(consts_map, \"Known Pixels Mask (Foreground or Background)\", cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matting Laplacian Construction <a name=\"matting-laplacian-construction\"></a>\n",
    "\n",
    "The **Matting Laplacian** is a key component of the Closed-Form Matting method proposed by Levin et al. It encodes the assumption that, within a small window, the colors of pixels lie approximately on a single line in RGB space. Based on this assumption, the Laplacian enforces that the estimated alpha values should vary smoothly in regions with homogeneous color, and should respect edges in the image.\n",
    "\n",
    "Formally, the Matting Laplacian $ L $ is a sparse $ N \\times N $ matrix (where $ N = H \\times W $) that penalizes differences in alpha values between similar pixels in a local window.\n",
    "\n",
    "### Window Definition\n",
    "\n",
    "For each pixel, we consider a local square window of size $ (2r+1) \\times (2r+1) $, typically with radius $ r = 1 $, i.e., a $ 3 \\times 3 $ window.\n",
    "\n",
    "For each window $ \\omega_k $, the local contribution to the Laplacian is:\n",
    "\n",
    "$$\n",
    "L_{ij}^{(k)} = \\delta_{ij} - \\frac{1}{|\\omega_k|} \\left( 1 + (I_i - \\mu_k)^T \\left( \\Sigma_k + \\frac{\\epsilon}{|\\omega_k|} I_3 \\right)^{-1} (I_j - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ I_i, I_j $: RGB values at pixels $ i $ and $ j $ within the window\n",
    "- $ \\mu_k $, $ \\Sigma_k $: mean and covariance of RGB values in the window\n",
    "- $ \\epsilon $: small regularization constant (e.g., $10^{-7}$)\n",
    "\n",
    "The total Laplacian is obtained by summing contributions over all overlapping windows.\n",
    "\n",
    "### Computation Notes\n",
    "- The Laplacian is implemented as a sparse matrix to scale to large images.\n",
    "- We skip windows that are entirely inside known foreground/background regions (optional optimization).\n",
    "\n",
    "We now define a function to compute the Laplacian matrix for a given image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def _rolling_block(A, block=(3, 3)):\n",
    "    \"\"\"Extracts all (3x3) windows from matrix A using strides.\"\"\"\n",
    "    shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
    "    strides = A.strides * 2\n",
    "    return as_strided(A, shape=shape, strides=strides)\n",
    "\n",
    "def compute_laplacian(img: np.ndarray, mask=None, eps: float =10**(-7), win_rad: int =1):\n",
    "    \"\"\"Computes Matting Laplacian for a given image.\n",
    "\n",
    "    Args:\n",
    "        img: 3-dim numpy matrix with input image\n",
    "        mask: mask of pixels for which Laplacian will be computed.\n",
    "            If not set Laplacian will be computed for all pixels.\n",
    "        eps: regularization parameter controlling alpha smoothness\n",
    "            from Eq. 12 of the original paper. Defaults to 1e-7.\n",
    "        win_rad: radius of window used to build Matting Laplacian (i.e.\n",
    "            radius of omega_k in Eq. 12).\n",
    "    Returns: sparse matrix holding Matting Laplacian.\n",
    "    \"\"\"\n",
    "\n",
    "    win_size = (win_rad * 2 + 1) ** 2\n",
    "    h, w, d = img.shape\n",
    "    # Number of window centre indices in h, w axes\n",
    "    c_h, c_w = h - 2 * win_rad, w - 2 * win_rad\n",
    "    win_diam = win_rad * 2 + 1\n",
    "\n",
    "    indsM = np.arange(h * w).reshape((h, w))\n",
    "    ravelImg = img.reshape(h * w, d)\n",
    "    win_inds = _rolling_block(indsM, block=(win_diam, win_diam))\n",
    "\n",
    "    win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
    "    if mask is not None:\n",
    "        mask = cv2.dilate(\n",
    "            mask.astype(np.uint8),\n",
    "            np.ones((win_diam, win_diam), np.uint8)\n",
    "        ).astype(bool)\n",
    "        win_mask = np.sum(mask.ravel()[win_inds], axis=2)\n",
    "        win_inds = win_inds[win_mask > 0, :]\n",
    "    else:\n",
    "        win_inds = win_inds.reshape(-1, win_size)\n",
    "\n",
    "    \n",
    "    winI = ravelImg[win_inds]\n",
    "\n",
    "    win_mu = np.mean(winI, axis=1, keepdims=True)\n",
    "    win_var = np.einsum('...ji,...jk ->...ik', winI, winI) / win_size - np.einsum('...ji,...jk ->...ik', win_mu, win_mu)\n",
    "\n",
    "    A = win_var + (eps/win_size)*np.eye(3)\n",
    "    B = (winI - win_mu).transpose(0, 2, 1)\n",
    "    X = np.linalg.solve(A, B).transpose(0, 2, 1)\n",
    "    vals = np.eye(win_size) - (1.0/win_size)*(1 + X @ B)\n",
    "\n",
    "    nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
    "    nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
    "    nz_indsVal = vals.ravel()\n",
    "    L = scipy.sparse.coo_matrix((nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h*w, h*w))\n",
    "\n",
    "    # rewrite L in CSR format\n",
    "    #L = scipy.sparse.csr_matrix((nz_indsVal, nz_indsCol, np.arange(0, nz_indsVal.shape[0] + 1, win_size)), shape=(h*w, h*w))\n",
    "    return L.tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Computing and Visualizing the Laplacian\n",
    "\n",
    "Once the Matting Laplacian construction function is in place, we can compute it for our specific image. To optimize performance and focus only on the unknown regions, we apply the Laplacian only to pixels where the alpha value is not already known from the scribbles (i.e., where the prior is equal to 0.5).\n",
    "\n",
    "This section also visualizes the sparsity structure of the resulting Laplacian matrix. This visualization provides insight into how local pixel interactions are encoded—each row in the Laplacian corresponds to a pixel, and its non-zero entries define which neighboring pixels influence its alpha value.\n",
    "\n",
    "A correctly formed Laplacian should:\n",
    "- Be symmetric and positive semi-definite\n",
    "- Contain non-zero entries mainly along and near the diagonal\n",
    "- Exhibit a repeating banded structure due to window-based neighborhood interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mask of unconstrained pixels (where alpha is unknown)\n",
    "unconstrained_mask = (prior == 0.5).astype(np.uint8)\n",
    "\n",
    "# Compute Laplacian only where needed\n",
    "L = compute_laplacian(image, mask=unconstrained_mask)\n",
    "\n",
    "print(f\"Laplacian shape: {L.shape}\")\n",
    "print(f\"Non-zero entries: {L.nnz:,}\")\n",
    "plt.spy(L, markersize=0.1)\n",
    "plt.title(\"Sparsity Pattern of Matting Laplacian\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large size and high sparsity of the Matting Laplacian, a full-matrix visualization can appear nearly empty except for the main diagonal. To better understand its internal structure, we can zoom into a small top-left portion of the matrix—for example, the first 1000×1000 entries.\n",
    "\n",
    "This zoomed view allows us to:\n",
    "\n",
    "- Confirm that each pixel is connected to its local neighbors (e.g., through 3×3 windows)\n",
    "- Observe the banded pattern around the diagonal\n",
    "- Validate that the Laplacian encodes local connectivity and smoothness assumptions as expected\n",
    "\n",
    "Keep in mind:\n",
    "- The diagonal represents each pixel’s connection to itself\n",
    "- The off-diagonal bands represent connections to nearby pixels (based on windowed neighborhoods)\n",
    "- The vertical distance between bands is approximately equal to the image width, due to raster row-major ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submatrix = L[:1000, :1000]\n",
    "plt.spy(submatrix, markersize=1)\n",
    "plt.title(\"Zoom into Top-Left 1000×1000\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solving for the Alpha Matte <a name=\"solving-for-the-alpha-matte\"></a>\n",
    "\n",
    "With the Matting Laplacian $ L $ constructed and the alpha prior $ \\alpha_0 $ defined from user scribbles, we now solve for the full alpha matte.\n",
    "\n",
    "We minimize the following quadratic energy:\n",
    "\n",
    "$$\n",
    "\\alpha^{*} = \\arg\\min_{\\alpha} \\; \\alpha^\\top L \\alpha + (\\alpha - \\alpha_0)^\\top C (\\alpha - \\alpha_0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ L $: the Matting Laplacian (enforces smoothness and local color consistency)\n",
    "- $ \\alpha_0 $: prior values (0, 0.5, or 1) derived from scribbles\n",
    "- $ C $: diagonal confidence matrix (high confidence in scribbled pixels, zero elsewhere)\n",
    "\n",
    "This results in a sparse linear system:\n",
    "\n",
    "$$\n",
    "(L + C)\\, \\alpha = C\\, \\alpha_0\n",
    "$$\n",
    "\n",
    "which we solve using a sparse direct solver.\n",
    "\n",
    "The solution is a real-valued alpha matte $ \\alpha(x, y) \\in [0, 1] $ that respects user annotations while smoothly interpolating unknown regions based on the image structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "# Flattened versions\n",
    "H, W = image.shape[:2]\n",
    "N = H * W\n",
    "\n",
    "# Confidence matrix: high (e.g. 100) where scribbles exist, zero elsewhere\n",
    "confidence = 100.0\n",
    "confidence_mask = (prior != 0.5).astype(np.float32)\n",
    "C = diags((confidence * confidence_mask).ravel(), format=\"csr\")\n",
    "\n",
    "# Right-hand side: C * alpha_0\n",
    "b = (confidence * confidence_mask * prior).ravel()\n",
    "\n",
    "# Solve the sparse linear system: (L + C) α = C α₀\n",
    "print(\"Solving sparse linear system...\")\n",
    "alpha_flat = spsolve(L + C, b)\n",
    "\n",
    "# Clip to [0, 1] and reshape to image\n",
    "alpha = np.clip(alpha_flat.reshape((H, W)), 0, 1)\n",
    "\n",
    "# Show result\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(alpha, \"Computed Alpha Matte\", cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Result Visualization and Analysis <a name=\"result-visualization-and-analysis\"></a>\n",
    "\n",
    "Once the alpha matte has been computed, we can visualize and analyze its quality in several ways:\n",
    "\n",
    "1. **View the alpha matte directly**: this shows soft transitions around object boundaries and how well the scribbles propagated.\n",
    "2. **Visualize the alpha overlayed on the original image**: this helps confirm whether alpha values align with object contours.\n",
    "3. **Composite the foreground over a new background**: for example, a solid white or transparent background to simulate cutout.\n",
    "\n",
    "This section provides practical tools to evaluate how well the alpha map separates foreground from background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple alpha blending: image * alpha + white * (1 - alpha)\n",
    "foreground_rgb = image * alpha[..., np.newaxis] + (1 - alpha[..., np.newaxis]) * 1.0\n",
    "foreground_rgb = foreground_rgb.astype(np.float32)\n",
    "\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(foreground_rgb, \"Foreground Blended Over White Background\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Further Reading <a name=\"conclusions-and-further-reading\"></a>\n",
    "\n",
    "In this notebook, we implemented and applied the **Closed-Form Matting** algorithm for natural images using sparse user annotations in the form of foreground/background scribbles.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The method computes a **soft alpha matte** $ \\alpha(x, y) \\in [0, 1] $, which smoothly separates foreground from background, especially around complex object boundaries.\n",
    "- Scribbles are interpreted as **hard constraints** that are propagated across the image using a **Matting Laplacian**, which encodes local color consistency.\n",
    "- Solving the system involves sparse linear algebra and benefits from the matrix's structure: sparse, symmetric, and positive semi-definite.\n",
    "- The resulting alpha matte can be directly used for **object extraction, compositing, or visual effects**.\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Requires minimal user input (a few scribbles).\n",
    "- Fully differentiable and fast (no iterative optimization).\n",
    "- Produces smooth and realistic mattes even for soft edges.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Assumes local color smoothness, which may fail in highly textured or low-contrast regions.\n",
    "- Sensitive to incorrect or sparse scribbles in ambiguous areas.\n",
    "- Not designed for real-time video or dynamic content without additional extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-Blurred Shape Recognition Playground\n",
    "\n",
    "## Exploring methods for recognizing and analyzing shapes from motion-blurred images  \n",
    "*Based on Caglioti and Giusti (2010)*\n",
    "\n",
    "- biagio.cancelliere@mail.polimi.it\n",
    "- davide1.franchi@mail.polimi.it\n",
    "- carlos2.ruiz@mail.polimi.it\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **[Introduction and Motivation](#introduction-and-motivation)**  \n",
    "2. **[Background and Theoretical Foundations](#background-and-theoretical-foundations)**  \n",
    "3. **[Dataset and Example Images](#dataset-and-example-images)**  \n",
    "4. **[Interactive Gradient Exploration: Sobel Filter](#interactive-gradient-exploration-sobel-filter)**  \n",
    "5. **[Interactive Edge Detection: Canny Filter](#interactive-edge-detection-canny-filter)**  \n",
    "6. **[Conclusions and Future Work](#conclusions-and-future-work)**  \n",
    "7. **[References](#references)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Natural image matting is the process of estimating the opacity (alpha value) of the foreground object in an image, separating it from the background. This task is essential in many applications, including image editing, film production, augmented reality, and compositing.\n",
    "\n",
    "The **alpha matte** is a real-valued mask $ \\alpha(x, y) \\in [0, 1] $ that indicates the proportion of foreground present at each pixel:\n",
    "- $ \\alpha = 1 $: full foreground\n",
    "- $ \\alpha = 0 $: full background\n",
    "- $ \\alpha \\in (0,1) $: partial or mixed pixels (e.g., along object boundaries or in regions of transparency)\n",
    "\n",
    "Estimating an accurate alpha matte from a single image is highly ill-posed. To resolve this ambiguity, additional constraints are needed. One widely adopted approach is to use **user-provided annotations**, such as:\n",
    "- **Scribbles** indicating known foreground and background pixels\n",
    "- **Trimaps** that mark definite foreground, background, and unknown regions\n",
    "\n",
    "In this notebook, we focus on the method proposed by **Levin et al. (2008)**, which formulates image matting as a **closed-form solution** by leveraging the local color distribution of the image. Their method constructs a **Matting Laplacian**, a sparse matrix that enforces smoothness of the alpha matte within small windows under a color line model.\n",
    "\n",
    "Given a small set of user-defined scribbles, the method solves a sparse linear system to propagate alpha values across the image, producing a soft, edge-preserving alpha matte.\n",
    "\n",
    "This notebook walks through the complete pipeline:\n",
    "- Interpreting user scribbles as alpha constraints\n",
    "- Constructing the matting Laplacian\n",
    "- Solving for the alpha matte via linear system optimization\n",
    "- Visualizing and interpreting the resulting alpha mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies <a name=\"setup-and-dependencies\"></a>\n",
    "\n",
    "This section installs and imports the necessary Python libraries for image processing, numerical computation, and visualization.\n",
    "\n",
    "Make sure to run this cell before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already available)\n",
    "# Uncomment the following line if running in a clean environment (e.g., Colab)\n",
    "# !pip install opencv-python-headless scipy matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure plots for consistent visual output\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# Utility for displaying images\n",
    "def imshow(img, title=None, cmap=None):\n",
    "    if img.ndim == 2:\n",
    "        plt.imshow(img, cmap=cmap or \"gray\")\n",
    "    else:\n",
    "        # OpenCV uses BGR; convert to RGB for display\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation <a name=\"data-preparation\"></a>\n",
    "\n",
    "To compute the alpha matte, we need two input images:\n",
    "\n",
    "- The **original RGB image** containing the object to be segmented.\n",
    "- A corresponding **scribbles image**, where the user has marked some pixels as definite foreground (typically white) and definite background (typically black).\n",
    "\n",
    "The scribbles act as soft constraints that guide the matting algorithm.\n",
    "\n",
    "The input and scribbles images must:\n",
    "- Have the same dimensions\n",
    "- Be in color (3-channel RGB/BGR)\n",
    "- Be properly aligned\n",
    "\n",
    "We will now load both images and display them to verify their correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the Input Image and Scribbles <a name=\"loading-the-input-image-and-scribbles\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "def _resize_pair(img: np.ndarray, scrib: np.ndarray, max_dim: Union[int, None]) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    If max_dim is not None and either dimension exceeds max_dim,\n",
    "    compute a scale factor and resize both img and scrib to that scale,\n",
    "    preserving aspect ratio. Returns (img_resized, scrib_resized, scale_factor).\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    if max_dim is None or max(h, w) <= max_dim:\n",
    "        return img, scrib, 1.0\n",
    "\n",
    "    scale = max_dim / max(h, w)\n",
    "    new_size = (int(w * scale), int(h * scale))\n",
    "    img_resized = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "    scrib_resized = cv2.resize(scrib, new_size, interpolation=cv2.INTER_AREA)\n",
    "    return img_resized, scrib_resized, scale\n",
    "\n",
    "\n",
    "\n",
    "# Load scraper image and scribbles\n",
    "image_path = \"../assets/images/IMG_1143.png\"\n",
    "scribbles_path = \"../assets/scribbles/flyingball.png.png\"\n",
    "\n",
    "# Load flyingball image and scribbles\n",
    "image_path = \"../assets/images/flyingball.png\"\n",
    "scribbles_path = \"../assets/scribbles/flyingball_scribbles.png\"\n",
    "\n",
    "# Load fan image and scribbles (invert the path definitions for changing images)\n",
    "image_path = \"../assets/images/IMG_9264.png\"\n",
    "scribbles_path = \"../assets/scribbles/IMG_9264_scribbles_hard.png\"\n",
    "\n",
    "image_raw = cv2.imread(image_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "scribbles_raw = cv2.imread(scribbles_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "\n",
    "# Sanity check\n",
    "assert image_raw is not None and scribbles_raw is not None, \"Failed to load input images\"\n",
    "assert image_raw.shape == scribbles_raw.shape, \"Image and scribbles must have the same dimensions\"\n",
    "\n",
    "# Resize if necessary\n",
    "max_dim = 512  # Set maximum size for either dimension\n",
    "image, scribbles, scale = _resize_pair(image_raw, scribbles_raw, max_dim)\n",
    "\n",
    "print(f\"Original shape: {image_raw.shape[:2]}, resized to: {image.shape[:2]}, scale factor: {scale:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing the Inputs <a name=\"visualizing-the-inputs\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original image\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(image, \"Original Image\")\n",
    "plt.show()\n",
    "\n",
    "# Display scribbles overlay\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(scribbles, \"Scribbles Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alpha Prior from Scribbles <a name=\"alpha-prior-from-scribbles\"></a>\n",
    "\n",
    "The matting algorithm requires an initial estimate of the alpha values—called the **alpha prior**—to guide the solution. This prior is derived directly from the scribbles image provided by the user.\n",
    "\n",
    "### Interpretation of Scribbles\n",
    "\n",
    "We assume the scribbles image consists of:\n",
    "- **Black strokes** (RGB ≈ [0, 0, 0]) marking definite background\n",
    "- **White strokes** (RGB ≈ [1, 1, 1]) marking definite foreground\n",
    "- **Unmodified areas** (same as original image) representing unknown regions\n",
    "\n",
    "We compute the alpha prior $\\alpha_0(x, y)$ for each pixel using the formula:\n",
    "\n",
    "$$\n",
    "\\alpha_0(x, y) = \\frac{1}{2} \\cdot \\text{sign} \\left( \\sum_{c=1}^{3} (S_c(x, y) - I_c(x, y)) \\right) + 0.5\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ S(x, y) $: the scribbles image\n",
    "- $ I(x, y) $: the original image\n",
    "- The result maps to:\n",
    "  - $ \\alpha_0 = 1 $: foreground (white scribble)\n",
    "  - $ \\alpha_0 = 0 $: background (black scribble)\n",
    "  - $ \\alpha_0 = 0.5 $: unknown (no scribble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alpha prior from scribbles\n",
    "prior = np.sign(np.sum(scribbles - image, axis=2)) / 2.0 + 0.5\n",
    "\n",
    "# Define constant mask: where prior is either 0 or 1 (i.e., where user actually painted)\n",
    "consts_map = (prior != 0.5).astype(np.float32)\n",
    "\n",
    "# Display prior\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(prior, \"Alpha Prior (from Scribbles)\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally visualize where scribbles exist (binary mask)\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(consts_map, \"Known Pixels Mask (Foreground or Background)\", cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matting Laplacian Construction <a name=\"matting-laplacian-construction\"></a>\n",
    "\n",
    "The **Matting Laplacian** is a key component of the Closed-Form Matting method proposed by Levin et al. It encodes the assumption that, within a small window, the colors of pixels lie approximately on a single line in RGB space. Based on this assumption, the Laplacian enforces that the estimated alpha values should vary smoothly in regions with homogeneous color, and should respect edges in the image.\n",
    "\n",
    "Formally, the Matting Laplacian $ L $ is a sparse $ N \\times N $ matrix (where $ N = H \\times W $) that penalizes differences in alpha values between similar pixels in a local window.\n",
    "\n",
    "### Window Definition\n",
    "\n",
    "For each pixel, we consider a local square window of size $ (2r+1) \\times (2r+1) $, typically with radius $ r = 1 $, i.e., a $ 3 \\times 3 $ window.\n",
    "\n",
    "For each window $ \\omega_k $, the local contribution to the Laplacian is:\n",
    "\n",
    "$$\n",
    "L_{ij}^{(k)} = \\delta_{ij} - \\frac{1}{|\\omega_k|} \\left( 1 + (I_i - \\mu_k)^T \\left( \\Sigma_k + \\frac{\\epsilon}{|\\omega_k|} I_3 \\right)^{-1} (I_j - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ I_i, I_j $: RGB values at pixels $ i $ and $ j $ within the window\n",
    "- $ \\mu_k $, $ \\Sigma_k $: mean and covariance of RGB values in the window\n",
    "- $ \\epsilon $: small regularization constant (e.g., $10^{-7}$)\n",
    "\n",
    "The total Laplacian is obtained by summing contributions over all overlapping windows.\n",
    "\n",
    "### Computation Notes\n",
    "- The Laplacian is implemented as a sparse matrix to scale to large images.\n",
    "- We skip windows that are entirely inside known foreground/background regions (optional optimization).\n",
    "\n",
    "We now define a function to compute the Laplacian matrix for a given image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def _rolling_block(A, block=(3, 3)):\n",
    "    \"\"\"Extracts all (3x3) windows from matrix A using strides.\"\"\"\n",
    "    shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
    "    strides = A.strides * 2\n",
    "    return as_strided(A, shape=shape, strides=strides)\n",
    "\n",
    "def compute_laplacian(img: np.ndarray, mask=None, eps: float =10**(-7), win_rad: int =1):\n",
    "    \"\"\"Computes Matting Laplacian for a given image.\n",
    "\n",
    "    Args:\n",
    "        img: 3-dim numpy matrix with input image\n",
    "        mask: mask of pixels for which Laplacian will be computed.\n",
    "            If not set Laplacian will be computed for all pixels.\n",
    "        eps: regularization parameter controlling alpha smoothness\n",
    "            from Eq. 12 of the original paper. Defaults to 1e-7.\n",
    "        win_rad: radius of window used to build Matting Laplacian (i.e.\n",
    "            radius of omega_k in Eq. 12).\n",
    "    Returns: sparse matrix holding Matting Laplacian.\n",
    "    \"\"\"\n",
    "\n",
    "    win_size = (win_rad * 2 + 1) ** 2\n",
    "    h, w, d = img.shape\n",
    "    # Number of window centre indices in h, w axes\n",
    "    c_h, c_w = h - 2 * win_rad, w - 2 * win_rad\n",
    "    win_diam = win_rad * 2 + 1\n",
    "\n",
    "    indsM = np.arange(h * w).reshape((h, w))\n",
    "    ravelImg = img.reshape(h * w, d)\n",
    "    win_inds = _rolling_block(indsM, block=(win_diam, win_diam))\n",
    "\n",
    "    win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
    "    if mask is not None:\n",
    "        mask = cv2.dilate(\n",
    "            mask.astype(np.uint8),\n",
    "            np.ones((win_diam, win_diam), np.uint8)\n",
    "        ).astype(bool)\n",
    "        win_mask = np.sum(mask.ravel()[win_inds], axis=2)\n",
    "        win_inds = win_inds[win_mask > 0, :]\n",
    "    else:\n",
    "        win_inds = win_inds.reshape(-1, win_size)\n",
    "\n",
    "    \n",
    "    winI = ravelImg[win_inds]\n",
    "\n",
    "    win_mu = np.mean(winI, axis=1, keepdims=True)\n",
    "    win_var = np.einsum('...ji,...jk ->...ik', winI, winI) / win_size - np.einsum('...ji,...jk ->...ik', win_mu, win_mu)\n",
    "\n",
    "    A = win_var + (eps/win_size)*np.eye(3)\n",
    "    B = (winI - win_mu).transpose(0, 2, 1)\n",
    "    X = np.linalg.solve(A, B).transpose(0, 2, 1)\n",
    "    vals = np.eye(win_size) - (1.0/win_size)*(1 + X @ B)\n",
    "\n",
    "    nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
    "    nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
    "    nz_indsVal = vals.ravel()\n",
    "    L = scipy.sparse.coo_matrix((nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h*w, h*w))\n",
    "\n",
    "    # rewrite L in CSR format\n",
    "    #L = scipy.sparse.csr_matrix((nz_indsVal, nz_indsCol, np.arange(0, nz_indsVal.shape[0] + 1, win_size)), shape=(h*w, h*w))\n",
    "    return L.tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Computing and Visualizing the Laplacian\n",
    "\n",
    "Once the Matting Laplacian construction function is in place, we can compute it for our specific image. To optimize performance and focus only on the unknown regions, we apply the Laplacian only to pixels where the alpha value is not already known from the scribbles (i.e., where the prior is equal to 0.5).\n",
    "\n",
    "This section also visualizes the sparsity structure of the resulting Laplacian matrix. This visualization provides insight into how local pixel interactions are encoded—each row in the Laplacian corresponds to a pixel, and its non-zero entries define which neighboring pixels influence its alpha value.\n",
    "\n",
    "A correctly formed Laplacian should:\n",
    "- Be symmetric and positive semi-definite\n",
    "- Contain non-zero entries mainly along and near the diagonal\n",
    "- Exhibit a repeating banded structure due to window-based neighborhood interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mask of unconstrained pixels (where alpha is unknown)\n",
    "unconstrained_mask = (prior == 0.5).astype(np.uint8)\n",
    "\n",
    "# Compute Laplacian only where needed\n",
    "L = compute_laplacian(image, mask=unconstrained_mask)\n",
    "\n",
    "print(f\"Laplacian shape: {L.shape}\")\n",
    "print(f\"Non-zero entries: {L.nnz:,}\")\n",
    "plt.spy(L, markersize=0.1)\n",
    "plt.title(\"Sparsity Pattern of Matting Laplacian\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large size and high sparsity of the Matting Laplacian, a full-matrix visualization can appear nearly empty except for the main diagonal. To better understand its internal structure, we can zoom into a small top-left portion of the matrix—for example, the first 1000×1000 entries.\n",
    "\n",
    "This zoomed view allows us to:\n",
    "\n",
    "- Confirm that each pixel is connected to its local neighbors (e.g., through 3×3 windows)\n",
    "- Observe the banded pattern around the diagonal\n",
    "- Validate that the Laplacian encodes local connectivity and smoothness assumptions as expected\n",
    "\n",
    "Keep in mind:\n",
    "- The diagonal represents each pixel’s connection to itself\n",
    "- The off-diagonal bands represent connections to nearby pixels (based on windowed neighborhoods)\n",
    "- The vertical distance between bands is approximately equal to the image width, due to raster row-major ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submatrix = L[:1000, :1000]\n",
    "plt.spy(submatrix, markersize=1)\n",
    "plt.title(\"Zoom into Top-Left 1000×1000\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solving for the Alpha Matte <a name=\"solving-for-the-alpha-matte\"></a>\n",
    "\n",
    "With the Matting Laplacian $ L $ constructed and the alpha prior $ \\alpha_0 $ defined from user scribbles, we now solve for the full alpha matte.\n",
    "\n",
    "We minimize the following quadratic energy:\n",
    "\n",
    "$$\n",
    "\\alpha^{*} = \\arg\\min_{\\alpha} \\; \\alpha^\\top L \\alpha + (\\alpha - \\alpha_0)^\\top C (\\alpha - \\alpha_0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ L $: the Matting Laplacian (enforces smoothness and local color consistency)\n",
    "- $ \\alpha_0 $: prior values (0, 0.5, or 1) derived from scribbles\n",
    "- $ C $: diagonal confidence matrix (high confidence in scribbled pixels, zero elsewhere)\n",
    "\n",
    "This results in a sparse linear system:\n",
    "\n",
    "$$\n",
    "(L + C)\\, \\alpha = C\\, \\alpha_0\n",
    "$$\n",
    "\n",
    "which we solve using a sparse direct solver.\n",
    "\n",
    "The solution is a real-valued alpha matte $ \\alpha(x, y) \\in [0, 1] $ that respects user annotations while smoothly interpolating unknown regions based on the image structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "# Flattened versions\n",
    "H, W = image.shape[:2]\n",
    "N = H * W\n",
    "\n",
    "# Confidence matrix: high (e.g. 100) where scribbles exist, zero elsewhere\n",
    "confidence = 100.0\n",
    "confidence_mask = (prior != 0.5).astype(np.float32)\n",
    "C = diags((confidence * confidence_mask).ravel(), format=\"csr\")\n",
    "\n",
    "# Right-hand side: C * alpha_0\n",
    "b = (confidence * confidence_mask * prior).ravel()\n",
    "\n",
    "# Solve the sparse linear system: (L + C) α = C α₀\n",
    "print(\"Solving sparse linear system...\")\n",
    "alpha_flat = spsolve(L + C, b)\n",
    "\n",
    "# Clip to [0, 1] and reshape to image\n",
    "alpha = np.clip(alpha_flat.reshape((H, W)), 0, 1)\n",
    "\n",
    "# Show result\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(alpha, \"Computed Alpha Matte\", cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Result Visualization and Analysis <a name=\"result-visualization-and-analysis\"></a>\n",
    "\n",
    "Once the alpha matte has been computed, we can visualize and analyze its quality in several ways:\n",
    "\n",
    "1. **View the alpha matte directly**: this shows soft transitions around object boundaries and how well the scribbles propagated.\n",
    "2. **Visualize the alpha overlayed on the original image**: this helps confirm whether alpha values align with object contours.\n",
    "3. **Composite the foreground over a new background**: for example, a solid white or transparent background to simulate cutout.\n",
    "\n",
    "This section provides practical tools to evaluate how well the alpha map separates foreground from background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple alpha blending: image * alpha + white * (1 - alpha)\n",
    "foreground_rgb = image * alpha[..., np.newaxis] + (1 - alpha[..., np.newaxis]) * 1.0\n",
    "foreground_rgb = foreground_rgb.astype(np.float32)\n",
    "\n",
    "plt.figure(figsize=(12.8, 9.6))\n",
    "imshow(foreground_rgb, \"Foreground Blended Over White Background\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Further Reading <a name=\"conclusions-and-further-reading\"></a>\n",
    "\n",
    "In this notebook, we implemented and applied the **Closed-Form Matting** algorithm for natural images using sparse user annotations in the form of foreground/background scribbles.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The method computes a **soft alpha matte** $ \\alpha(x, y) \\in [0, 1] $, which smoothly separates foreground from background, especially around complex object boundaries.\n",
    "- Scribbles are interpreted as **hard constraints** that are propagated across the image using a **Matting Laplacian**, which encodes local color consistency.\n",
    "- Solving the system involves sparse linear algebra and benefits from the matrix's structure: sparse, symmetric, and positive semi-definite.\n",
    "- The resulting alpha matte can be directly used for **object extraction, compositing, or visual effects**.\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Requires minimal user input (a few scribbles).\n",
    "- Fully differentiable and fast (no iterative optimization).\n",
    "- Produces smooth and realistic mattes even for soft edges.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Assumes local color smoothness, which may fail in highly textured or low-contrast regions.\n",
    "- Sensitive to incorrect or sparse scribbles in ambiguous areas.\n",
    "- Not designed for real-time video or dynamic content without additional extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Motivation\n",
    "\n",
    "### What information does motion blur provide?\n",
    "\n",
    "Motion blur occurs when an object moves during the camera's exposure period, creating a visible smear in the captured image. While traditionally considered a limitation in photography, blurred smears can actually encode valuable information about the object's shape and motion characteristics.\n",
    "\n",
    "The original research by Caglioti and Giusti (2010) leverages these blurred smears to extract detailed geometric and temporal information, such as the object's initial and final contours, motion trajectories, and even intermediate shapes. This notebook aims to explore practical methods inspired by their approach, demonstrating how we can effectively recognize and reconstruct shapes directly from blurred images, thus turning a common photographic artifact into a useful source of visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Background and Theoretical Foundations\n",
    "\n",
    "### Motion blur, alpha matting, and geometric interpretation\n",
    "\n",
    "When an object moves during a camera's exposure time, its projection sweeps across the image plane, resulting in a motion-blurred appearance. This blurred region—called a *smear*—can be interpreted as a semitransparent layer, where the **alpha value** at each pixel corresponds to the fraction of time the object covered that pixel.\n",
    "\n",
    "The image formation process in this context can be modeled using the **Porter-Duff alpha compositing equation**:\n",
    "\n",
    "$$\n",
    "C(p) = \\alpha(p) \\cdot o(p) + (1 - \\alpha(p)) \\cdot B(p)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C(p) &\\quad \\text{is the observed intensity at pixel } p \\\\\\\\\n",
    "o(p) &\\quad \\text{is the object’s intensity} \\\\\\\\\n",
    "B(p) &\\quad \\text{is the background intensity} \\\\\\\\\n",
    "\\alpha(p) &\\in [0, 1] \\quad \\text{is the proportion of exposure during which the object covers } p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The gradient of the alpha matte, denoted as $ \\nabla \\alpha $\n",
    "contains crucial structural cues:\n",
    "\n",
    "- Discontinuities in $ \\nabla \\alpha $ align with changes in the object’s apparent contour.\n",
    "- Iso-alpha curves trace the contour at specific instants within the exposure.\n",
    "\n",
    "This theoretical framework allows one to:\n",
    "\n",
    "- Identify the object's initial and final contours.\n",
    "- Estimate envelopes of motion.\n",
    "- Infer local motion dynamics (e.g., impacts, corners, speed changes).\n",
    "\n",
    "Understanding these principles forms the foundation for the shape recognition techniques implemented in the rest of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and Example Images\n",
    "\n",
    "### Description of the image (`IMG_9264.png`)\n",
    "\n",
    "To explore motion-blurred shape recognition techniques, we work with a set of sample images captured under controlled motion conditions. The primary image used in this notebook, `IMG_9264.png`, belongs to a sequence in which a ceiling fan moves rapidly across the scene, producing a visible motion blur. It exhibits characteristic streaks and transparency variations caused by object displacement during exposure, making it a suitable candidate for testing shape extraction methods.\n",
    "\n",
    "Before applying any filters or computational steps, we load and visualize the image to assess its resolution, structure, and blur properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"../assets/images/IMG_9264.png\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"Original motion-blurred image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional example: `flyingball.png`\n",
    "\n",
    "The image `flyingball.png` shows a dark spherical object in motion against a flat, uniform background. The object appears blurred along its motion direction, resulting in a soft-edged contour and a semi-transparent gradient toward the motion trail. This type of image is particularly relevant for testing the extraction of iso-alpha curves, envelope contours, and identifying region boundaries without the need for full alpha matte recovery. Its high contrast and simple background make it ideal for gradient-based techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"../assets/images/flyingball.png\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"Original motion-blurred image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Gradient Exploration: Sobel Filter\n",
    "\n",
    "### Identifying contours with Sobel filters\n",
    "\n",
    "This section introduces an interactive visualization tool for analyzing how different gradient-based parameters affect contour detection in motion-blurred images.\n",
    "\n",
    "By applying the Sobel operator to a grayscale version of the input image, we compute the gradient magnitude, which highlights regions with strong intensity changes—typically corresponding to edges and boundaries of the moving object. Before applying the Sobel filter, a Gaussian blur is optionally used to suppress high-frequency noise.\n",
    "\n",
    "The interface allows the user to experiment with:\n",
    "- the size and standard deviation of the Gaussian kernel,\n",
    "- the derivative order in the x and y directions,\n",
    "- the size of the Sobel kernel,\n",
    "- normalization and inversion of the result,\n",
    "- optional thresholding to produce a binary edge map,\n",
    "- display options such as interpolation method and colormap.\n",
    "\n",
    "This interactive approach supports visual intuition about the role of each parameter, and helps to identify suitable configurations for emphasizing features such as contour boundaries, envelope shapes, and iso-gradient regions in motion-blurred objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Checkbox, Dropdown\n",
    "\n",
    "def show_gradient_interactive(\n",
    "    gaussian_kernel: int = 5,\n",
    "    gaussian_sigma: float = 1.0,\n",
    "    sobel_dx: int = 1,\n",
    "    sobel_dy: int = 1,\n",
    "    sobel_ksize: int = 3,\n",
    "    scale: float = 0.4,\n",
    "    normalize: bool = True,\n",
    "    invert: bool = True,\n",
    "    umbralize: bool = False,\n",
    "    interpolation: str = 'nearest',\n",
    "    colormap: str = 'gray',\n",
    "    dpi: int = 100,\n",
    "    thresh_val: int = 250\n",
    "):\n",
    "    # 1) Leer y escalar\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    h, w = img.shape\n",
    "    if scale != 1.0:\n",
    "        img = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 2) Gaussian Blur\n",
    "    img_blur = cv2.GaussianBlur(img, (gaussian_kernel, gaussian_kernel), gaussian_sigma)\n",
    "\n",
    "    # 3) Sobel en x e y\n",
    "    gx = cv2.Sobel(img_blur, cv2.CV_64F, sobel_dx, 0, ksize=sobel_ksize)\n",
    "    gy = cv2.Sobel(img_blur, cv2.CV_64F, 0, sobel_dy, ksize=sobel_ksize)\n",
    "    mag = np.hypot(gx, gy)\n",
    "\n",
    "    # 4) Normalizar a [0,255]\n",
    "    if normalize:\n",
    "        mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    mag = mag.astype(np.uint8)\n",
    "\n",
    "    # 5) Invertir (opcional)\n",
    "    if invert:\n",
    "        mag = cv2.bitwise_not(mag)\n",
    "\n",
    "    if umbralize:\n",
    "        # 6) Umbralizar usando el valor ajustable\n",
    "        _, mag = cv2.threshold(mag, thresh_val, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 7) Mostrar resultado umbralizado\n",
    "    plt.figure(figsize=(img.shape[1]/dpi, img.shape[0]/dpi), dpi=dpi)\n",
    "    plt.imshow(mag, cmap=colormap, vmin=0, vmax=255, interpolation=interpolation)\n",
    "    plt.title(\n",
    "        f\"Gk={gaussian_kernel} σ={gaussian_sigma:.1f} | \"\n",
    "        f\"Sx={sobel_dx} Sy={sobel_dy} k={sobel_ksize} | \"\n",
    "        f\"Threshold={thresh_val} | scale={scale} dpi={dpi}\"\n",
    "    )\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../assets/images/IMG_9264.png\"\n",
    "\n",
    "interact(\n",
    "    show_gradient_interactive,\n",
    "    gaussian_kernel=IntSlider(13, min=1, max=31, step=2, description='Gauss k'),\n",
    "    gaussian_sigma=FloatSlider(1.0, min=0.0, max=10.0, step=0.5, description='σ Gauss'),\n",
    "    sobel_dx=IntSlider(1, min=0, max=2, step=1, description='Sobel dx'),\n",
    "    sobel_dy=IntSlider(1, min=0, max=2, step=1, description='Sobel dy'),\n",
    "    sobel_ksize=IntSlider(25, min=1, max=31, step=2, description='Sobel k'),\n",
    "    scale=FloatSlider(0.4, min=0.1, max=1.0, step=0.1, description='Scale'),\n",
    "    normalize=Checkbox(True, description='Normalize'),\n",
    "    invert=Checkbox(True, description='Invert'),\n",
    "    umbralize=Checkbox(True, description='Umbralize'),\n",
    "    interpolation=Dropdown(\n",
    "        options=['nearest', 'bilinear', 'bicubic', 'antialiased'],\n",
    "        value='nearest',\n",
    "        description='Interp.'\n",
    "    ),\n",
    "    colormap=Dropdown(\n",
    "        options=['gray', 'viridis', 'hot', 'magma', 'inferno'],\n",
    "        value='gray',\n",
    "        description='Colormap'\n",
    "    ),\n",
    "    dpi=IntSlider(100, min=50, max=300, step=50, description='DPI'),\n",
    "    thresh_val=IntSlider(239, min=150, max=255, step=1, description='Threshold')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../assets/images/IMG_9264.png\"\n",
    "\n",
    "interact(\n",
    "    show_gradient_interactive,\n",
    "    gaussian_kernel=IntSlider(13, min=1, max=31, step=2, description='Gauss k'),\n",
    "    gaussian_sigma=FloatSlider(1.0, min=0.0, max=10.0, step=0.5, description='σ Gauss'),\n",
    "    sobel_dx=IntSlider(1, min=0, max=2, step=1, description='Sobel dx'),\n",
    "    sobel_dy=IntSlider(1, min=0, max=2, step=1, description='Sobel dy'),\n",
    "    sobel_ksize=IntSlider(25, min=1, max=31, step=2, description='Sobel k'),\n",
    "    scale=FloatSlider(0.4, min=0.1, max=1.0, step=0.1, description='Scale'),\n",
    "    normalize=Checkbox(True, description='Normalize'),\n",
    "    invert=Checkbox(True, description='Invert'),\n",
    "    umbralize=Checkbox(False, description='Umbralize'),\n",
    "    interpolation=Dropdown(\n",
    "        options=['nearest', 'bilinear', 'bicubic', 'antialiased'],\n",
    "        value='nearest',\n",
    "        description='Interp.'\n",
    "    ),\n",
    "    colormap=Dropdown(\n",
    "        options=['gray', 'viridis', 'hot', 'magma', 'inferno'],\n",
    "        value='gray',\n",
    "        description='Colormap'\n",
    "    ),\n",
    "    dpi=IntSlider(100, min=50, max=300, step=50, description='DPI'),\n",
    "    thresh_val=IntSlider(239, min=150, max=255, step=1, description='Threshold')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../assets/images/flyingball.png\"\n",
    "\n",
    "interact(\n",
    "    show_gradient_interactive,\n",
    "    gaussian_kernel=IntSlider(1, min=1, max=31, step=2, description='Gauss k'),\n",
    "    gaussian_sigma=FloatSlider(1.0, min=0.0, max=10.0, step=0.5, description='σ Gauss'),\n",
    "    sobel_dx=IntSlider(1, min=0, max=2, step=1, description='Sobel dx'),\n",
    "    sobel_dy=IntSlider(1, min=0, max=2, step=1, description='Sobel dy'),\n",
    "    sobel_ksize=IntSlider(9, min=1, max=31, step=2, description='Sobel k'),\n",
    "    scale=FloatSlider(0.6, min=0.1, max=1.0, step=0.1, description='Scale'),\n",
    "    normalize=Checkbox(True, description='Normalize'),\n",
    "    invert=Checkbox(True, description='Invert'),\n",
    "    umbralize=Checkbox(False, description='Umbralize'),\n",
    "    interpolation=Dropdown(\n",
    "        options=['nearest', 'bilinear', 'bicubic', 'antialiased'],\n",
    "        value='nearest',\n",
    "        description='Interp.'\n",
    "    ),\n",
    "    colormap=Dropdown(\n",
    "        options=['gray', 'viridis', 'hot', 'magma', 'inferno'],\n",
    "        value='gray',\n",
    "        description='Colormap'\n",
    "    ),\n",
    "    dpi=IntSlider(100, min=50, max=300, step=50, description='DPI'),\n",
    "    thresh_val=IntSlider(239, min=150, max=255, step=1, description='Threshold')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Edge Detection: Canny Filter\n",
    "\n",
    "### Edge detection comparisons\n",
    "\n",
    "In this section, we explore the application of the **Canny edge detector** as a tool for contour extraction in motion-blurred images.\n",
    "\n",
    "Canny filtering is particularly effective at highlighting well-defined edges while suppressing noise, making it a strong candidate for identifying the outer boundaries of fast-moving objects captured with long exposure. By adjusting its parameters interactively—such as the low and high thresholds, Gaussian smoothing, and aperture size—we can observe how different configurations affect the visibility and continuity of the resulting contours.\n",
    "\n",
    "Compared to gradient magnitude visualizations (as seen in the Sobel-based analysis in Section 4), the Canny filter provides a binary edge map that is often more selective, but may fail to capture subtle structures present in smoother transitions of blur. This makes it especially valuable for detecting:\n",
    "- the initial and final apparent contours of a moving object,\n",
    "- sharp structural transitions within the blur smear,\n",
    "- high-contrast envelope boundaries.\n",
    "\n",
    "Through interactive tuning, users can assess the filter's effectiveness and develop an intuition for its strengths and limitations in shape recovery tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Checkbox, Dropdown\n",
    "\n",
    "def show_canny_interactive(\n",
    "    gaussian_kernel: int = 5,\n",
    "    gaussian_sigma: float = 1.0,\n",
    "    low_thresh: int = 50,\n",
    "    high_thresh: int = 150,\n",
    "    aperture_size: int = 3,\n",
    "    L2gradient: bool = False,\n",
    "    scale: float = 1.0,\n",
    "    interpolation: str = 'nearest',\n",
    "    colormap: str = 'gray',\n",
    "    dpi: int = 100,\n",
    "    invert: bool = True\n",
    "):\n",
    "    # 1) Leer y escalar\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    h, w = img.shape\n",
    "    if scale != 1.0:\n",
    "        img = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "        h, w = img.shape\n",
    "\n",
    "    # 2) Suavizado Gaussiano (kernel impar)\n",
    "    gk = gaussian_kernel | 1\n",
    "    blur = cv2.GaussianBlur(img, (gk, gk), gaussian_sigma)\n",
    "\n",
    "    # 3) Canny\n",
    "    # aperture_size debe ser 3,5,7; forzamos impar y >=3\n",
    "    ap = max(3, aperture_size | 1)\n",
    "    canny = cv2.Canny(blur, low_thresh, high_thresh,\n",
    "                      apertureSize=ap,\n",
    "                      L2gradient=L2gradient)\n",
    "    \n",
    "    # 5) Invertir (opcional)\n",
    "    if invert:\n",
    "        canny = cv2.bitwise_not(canny)\n",
    "\n",
    "    # 4) Mostrar sin márgenes\n",
    "    fig = plt.figure(figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(canny, cmap=colormap,\n",
    "              vmin=0, vmax=255,\n",
    "              interpolation=interpolation)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Gauss k={gk} σ={gaussian_sigma:.1f} | \"\n",
    "                 f\"Canny low={low_thresh} high={high_thresh} | \"\n",
    "                 f\"ap={ap} L2={L2gradient}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../assets/images/IMG_9264.png\"\n",
    "\n",
    "interact(\n",
    "    show_canny_interactive,\n",
    "    gaussian_kernel=IntSlider(27,   min=1,  max=31, step=2, description='Gauss k'),\n",
    "    gaussian_sigma=FloatSlider(2.0, min=0.0,max=10.0,step=0.5,description='σ Gauss'),\n",
    "    low_thresh=IntSlider(255,  min=0,  max=255, step=1, description='Low Th'),\n",
    "    high_thresh=IntSlider(255, min=0,  max=255, step=1, description='High Th'),\n",
    "    aperture_size=IntSlider(5, min=3,  max=7,   step=1, description='Aperture'),\n",
    "    L2gradient=Checkbox(True, description='L2 grad'),\n",
    "    scale=FloatSlider(0.3, min=0.1, max=1.0, step=0.1, description='Scale'),\n",
    "    interpolation=Dropdown(\n",
    "        options=['nearest','bilinear','bicubic','antialiased'],\n",
    "        value='nearest', description='Interp.'\n",
    "    ),\n",
    "    colormap=Dropdown(\n",
    "        options=['gray','viridis','hot','magma','inferno'],\n",
    "        value='gray', description='Colormap'\n",
    "    ),\n",
    "    dpi=IntSlider(100, min=50, max=300, step=50, description='DPI')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "In this notebook, we explored the problem of shape recognition in motion-blurred images by analyzing gradient-based features and applying classical edge detection techniques. Building on the theoretical framework presented by Caglioti and Giusti (2010), we demonstrated that a significant amount of spatial and motion-related information can be extracted from a single blurred image—even without explicitly computing the alpha matte.\n",
    "\n",
    "Through interactive experiments with Sobel and Canny filters, we showed how parameter tuning influences the visibility and continuity of object contours. These visualizations allowed us to:\n",
    "- Approximate initial and final contours of the moving object.\n",
    "- Identify envelope boundaries and gradient discontinuities.\n",
    "\n",
    "While the absence of a true alpha matte limits the interpretability of finer motion structure, the use of gradient magnitude and edge detection provides a lightweight and practical method for extracting meaningful geometric features from motion-blurred imagery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
